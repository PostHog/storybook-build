{"version":3,"file":"89913.0303023b.iframe.bundle.js","mappings":";;;;;;;;;AA2DA;AACA;;;;;;AAMA;AACA;;;AAGA;AACA;AACA;AACA;;;AAGA;;;ACTA;;;;AAIA","sources":["webpack://@posthog/storybook/../../frontend/src/scenes/hog-functions/logs/logsViewerLogic.tsx","webpack://@posthog/storybook/../../products/messaging/frontend/Campaigns/campaignMetricsLogic.ts"],"sourcesContent":["import { actions, events, kea, key, listeners, path, props, reducers, selectors } from 'kea'\nimport { loaders } from 'kea-loaders'\nimport api from 'lib/api'\nimport { Dayjs, dayjs } from 'lib/dayjs'\n\nimport { hogql } from '~/queries/utils'\nimport { LogEntryLevel } from '~/types'\n\nimport type { logsViewerLogicType } from './logsViewerLogicType'\n\nexport const ALL_LOG_LEVELS: LogEntryLevel[] = ['DEBUG', 'LOG', 'INFO', 'WARNING', 'ERROR']\nexport const DEFAULT_LOG_LEVELS: LogEntryLevel[] = ['LOG', 'INFO', 'WARNING', 'ERROR']\n\nexport type LogsViewerLogicProps = {\n    sourceType: 'hog_function' | 'hog_flow'\n    sourceId: string\n}\n\nexport type LogsViewerFilters = {\n    levels: LogEntryLevel[]\n    search: string\n    date_from?: string\n    date_to?: string\n}\n\nexport const LOG_VIEWER_LIMIT = 500\n\nexport type GroupedLogEntry = {\n    instanceId: string\n    maxTimestamp: Dayjs\n    minTimestamp: Dayjs\n    logLevel: LogEntryLevel\n    entries: {\n        message: string\n        level: LogEntryLevel\n        timestamp: Dayjs\n    }[]\n}\n\ntype GroupedLogEntryRequest = {\n    sourceType: 'hog_function' | 'hog_flow'\n    sourceId: string\n    levels: LogEntryLevel[]\n    search: string\n    date_from?: string\n    date_to?: string\n    order: 'ASC' | 'DESC'\n}\n\nconst loadGroupedLogs = async (request: GroupedLogEntryRequest): Promise<GroupedLogEntry[]> => {\n    const query = hogql`\n        SELECT\n            instance_id,\n            max(timestamp) AS latest_timestamp,\n            min(timestamp) AS earliest_timestamp,\n            arraySort(\n                groupArray((timestamp, level, message))\n            ) AS messages\n        FROM log_entries\n        WHERE log_source = ${request.sourceType}\n        AND log_source_id = ${request.sourceId}\n        AND timestamp > {filters.dateRange.from}\n        AND timestamp < {filters.dateRange.to}\n        AND instance_id in (\n            SELECT DISTINCT instance_id\n            FROM log_entries\n            WHERE log_source = ${request.sourceType}\n            AND log_source_id = ${request.sourceId}\n            AND timestamp > {filters.dateRange.from}\n            AND timestamp < {filters.dateRange.to}\n            AND lower(level) IN (${hogql.raw(request.levels.map((level) => `'${level.toLowerCase()}'`).join(','))})\n            AND message ILIKE '%${hogql.raw(request.search)}%'\n            ORDER BY timestamp ${hogql.raw(request.order)}\n            LIMIT ${LOG_VIEWER_LIMIT}\n        )\n        GROUP BY instance_id\n        ORDER BY latest_timestamp DESC`\n\n    const response = await api.queryHogQL(query, {\n        refresh: 'force_blocking',\n        filtersOverride: {\n            date_from: request.date_from ?? '-7d',\n            date_to: request.date_to,\n        },\n    })\n\n    return response.results.map((result) => ({\n        instanceId: result[0],\n        maxTimestamp: dayjs(result[1]),\n        minTimestamp: dayjs(result[2]),\n        entries: result[3].map((entry: any) => ({\n            timestamp: dayjs(entry[0]),\n            level: entry[1].toUpperCase(),\n            message: entry[2],\n        })),\n    })) as GroupedLogEntry[]\n}\n\nconst sanitizeGroupedLogs = (groups: GroupedLogEntry[]): GroupedLogEntry[] => {\n    const byId: Record<string, GroupedLogEntry> = {}\n\n    for (const group of groups) {\n        // Set the group if not already set\n        if (!byId[group.instanceId]) {\n            byId[group.instanceId] = group\n        } else {\n            // If the group already exists, we need to merge the entries\n            for (const entry of group.entries) {\n                if (!byId[group.instanceId].entries.find((e) => e.timestamp.isSame(entry.timestamp))) {\n                    byId[group.instanceId].entries.push(entry)\n                }\n            }\n        }\n\n        // Sort the entries by timestamp\n        byId[group.instanceId].entries.sort((a, b) => a.timestamp.diff(b.timestamp))\n\n        // Go in reverse and find the highest level message\n\n        const highestLogLevel = group.entries.reduce((max, entry) => {\n            return Math.max(max, ALL_LOG_LEVELS.indexOf(entry.level))\n        }, 0)\n        byId[group.instanceId].logLevel = ALL_LOG_LEVELS[highestLogLevel]\n    }\n\n    return Object.values(byId).sort((a, b) => b.maxTimestamp.diff(a.maxTimestamp))\n}\n\nexport const logsViewerLogic = kea<logsViewerLogicType>([\n    path((key) => ['scenes', 'pipeline', 'hogfunctions', 'logs', 'logsViewerLogic', key]),\n    props({} as LogsViewerLogicProps), // TODO: Remove `stage` from props, it isn't needed here for anything\n    key(({ sourceType, sourceId }) => `${sourceType}:${sourceId}`),\n    actions({\n        setFilters: (filters: Partial<LogsViewerFilters>) => ({ filters }),\n        addLogGroups: (logGroups: GroupedLogEntry[]) => ({ logGroups }),\n        setHiddenLogs: (logGroups: GroupedLogEntry[]) => ({ logGroups }),\n        clearHiddenLogs: true,\n        markLogsEnd: true,\n        revealHiddenLogs: true,\n        setRowExpanded: (instanceId: string, expanded: boolean) => ({ instanceId, expanded }),\n        scheduleLoadNewerLogs: true,\n        loadLogs: true,\n        loadNewerLogs: true,\n    }),\n    loaders(({ props, values, actions }) => ({\n        logs: [\n            [] as GroupedLogEntry[],\n            {\n                loadLogs: async (_, breakpoint) => {\n                    await breakpoint(10)\n\n                    actions.clearHiddenLogs()\n\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_from: values.filters.date_from,\n                        date_to: values.filters.date_to,\n                        order: 'DESC',\n                    }\n                    const results = await loadGroupedLogs(logParams)\n\n                    await breakpoint(10)\n\n                    return sanitizeGroupedLogs(results)\n                },\n                loadMoreLogs: async () => {\n                    if (!values.oldestLogTimestamp) {\n                        return values.logs\n                    }\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_to: values.oldestLogTimestamp.toISOString(),\n                        date_from: values.filters.date_from,\n                        order: 'DESC',\n                    }\n\n                    const results = await loadGroupedLogs(logParams)\n\n                    if (!results.length) {\n                        actions.markLogsEnd()\n                    }\n                    return sanitizeGroupedLogs([...results, ...values.logs])\n                },\n\n                revealHiddenLogs: () => {\n                    // We pull out the hidden log groups and add them to the main logs\n                    const hiddenLogs = [...values.hiddenLogs]\n\n                    actions.clearHiddenLogs()\n                    return sanitizeGroupedLogs([...hiddenLogs, ...values.logs])\n                },\n                addLogGroups: ({ logGroups }) => {\n                    return sanitizeGroupedLogs([...logGroups, ...values.logs])\n                },\n            },\n        ],\n\n        hiddenLogs: [\n            [] as GroupedLogEntry[],\n            {\n                loadNewerLogs: async (_, breakpoint) => {\n                    await breakpoint(10)\n\n                    // We load all logs groups that have a timestamp after the newest log timestamp\n                    // For ones we already have we just replace them, otherwise we add them to the \"hidden\" logs list\n                    if (!values.newestLogTimestamp) {\n                        return values.hiddenLogs\n                    }\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_from: values.newestLogTimestamp.toISOString(),\n                        date_to: values.filters.date_to,\n                        order: 'ASC',\n                    }\n\n                    const results = await loadGroupedLogs(logParams)\n\n                    await breakpoint(10)\n\n                    const newLogs: GroupedLogEntry[] = []\n                    const existingLogsToUpdate: GroupedLogEntry[] = []\n                    const existingLogIds = values.logs.map((log) => log.instanceId)\n\n                    if (values.logsLoading) {\n                        // TRICKY: Something changed whilst we were doing this query - we don't want to mess with things\n                        // so we just exit\n                        return values.hiddenLogs\n                    }\n\n                    for (const log of results) {\n                        if (existingLogIds.includes(log.instanceId)) {\n                            // If we already have this log group showing then we can just update it\n                            existingLogsToUpdate.push(log)\n                        } else {\n                            // Otherwise we add it to the list of hidden logs\n                            newLogs.push(log)\n                        }\n                    }\n\n                    if (existingLogsToUpdate.length) {\n                        // Update the existing logs with the new data\n                        actions.loadLogsSuccess(sanitizeGroupedLogs([...existingLogsToUpdate, ...values.logs]))\n                    }\n\n                    actions.scheduleLoadNewerLogs()\n\n                    return sanitizeGroupedLogs([...newLogs, ...values.hiddenLogs])\n                },\n                clearHiddenLogs: () => [],\n            },\n        ],\n    })),\n    reducers({\n        filters: [\n            {\n                search: '',\n                levels: DEFAULT_LOG_LEVELS,\n                date_from: '-7d',\n                date_to: undefined,\n            } as LogsViewerFilters,\n            {\n                setFilters: (state, { filters }) => ({\n                    ...state,\n                    ...filters,\n                }),\n            },\n        ],\n        isThereMoreToLoad: [\n            true,\n            {\n                markLogsEnd: () => false,\n                loadLogs: () => true,\n            },\n        ],\n        expandedRows: [\n            {} as Record<string, boolean>,\n            {\n                setRowExpanded: (state, { instanceId, expanded }) => ({\n                    ...state,\n                    [instanceId]: expanded,\n                }),\n            },\n        ],\n    }),\n    selectors(() => ({\n        newestLogTimestamp: [\n            (s) => [s.logs, s.hiddenLogs],\n            (logs: GroupedLogEntry[], hiddenLogs: GroupedLogEntry[]): Dayjs | null => {\n                return logs.concat(hiddenLogs).reduce((max, log) => {\n                    if (!max) {\n                        return log.maxTimestamp\n                    }\n                    return log.maxTimestamp.isAfter(max) ? log.maxTimestamp : max\n                }, null as Dayjs | null)\n            },\n        ],\n\n        oldestLogTimestamp: [\n            (s) => [s.logs, s.hiddenLogs],\n            (logs: GroupedLogEntry[], hiddenLogs: GroupedLogEntry[]): Dayjs | null => {\n                return logs.concat(hiddenLogs).reduce((min, log) => {\n                    if (!min) {\n                        return log.minTimestamp\n                    }\n                    return log.minTimestamp.isBefore(min) ? log.minTimestamp : min\n                }, null as Dayjs | null)\n            },\n        ],\n    })),\n    listeners(({ actions, cache }) => ({\n        setFilters: async (_, breakpoint) => {\n            await breakpoint(500)\n            actions.loadLogs()\n        },\n\n        loadLogsSuccess: () => {\n            actions.scheduleLoadNewerLogs()\n        },\n\n        scheduleLoadNewerLogs: () => {\n            if (cache.pollingTimeout) {\n                clearTimeout(cache.pollingTimeout)\n            }\n            cache.pollingTimeout = setTimeout(() => actions.loadNewerLogs(), 5000)\n        },\n    })),\n    events(({ actions, cache }) => ({\n        afterMount: () => {\n            actions.loadLogs()\n        },\n        beforeUnmount: () => {\n            clearInterval(cache.pollingTimeout)\n        },\n    })),\n])\n","import { actions, kea, key, listeners, path, props, reducers } from 'kea'\nimport { loaders } from 'kea-loaders'\nimport api from 'lib/api'\nimport { MetricsFilters } from 'scenes/hog-functions/metrics/hogFunctionMetricsLogic'\n\nimport { hogql } from '~/queries/utils'\n\nimport type { campaignMetricsLogicType } from './campaignMetricsLogicType'\n\nexport type CampaignMetricsLogicProps = {\n    id: string\n}\n\nexport const ALL_METRIC_TYPES = [\n    { label: 'Succeeded', value: 'succeeded' },\n    { label: 'Failed', value: 'failed' },\n    { label: 'Filtered', value: 'filtered' },\n    { label: 'Disabled temporarily', value: 'disabled_temporarily' },\n    { label: 'Disabled permanently', value: 'disabled_permanently' },\n    { label: 'Masked', value: 'masked' },\n    { label: 'Filtering failed', value: 'filtering_failed' },\n    { label: 'Inputs failed', value: 'inputs_failed' },\n    { label: 'Fetch', value: 'fetch' },\n]\n\nconst DEFAULT_FILTERS: MetricsFilters = {\n    before: undefined,\n    after: '-7d',\n    interval: 'day',\n    name: ALL_METRIC_TYPES.filter(({ value }) => value !== 'filtered')\n        .map(({ value }) => value)\n        .join(','),\n}\n\nexport type CampaignMetricsDetails = {\n    name: string\n    labels: string[]\n    values: number[]\n    total: number\n}\n\nexport const campaignMetricsLogic = kea<campaignMetricsLogicType>([\n    props({} as CampaignMetricsLogicProps),\n    key(({ id }: CampaignMetricsLogicProps) => id),\n    path((id) => ['messaging', 'campaigns', 'campaignMetricsLogic', id]),\n    actions({\n        setFilters: (filters: Partial<MetricsFilters>) => ({ filters }),\n    }),\n    loaders(({ values, props }) => ({\n        metricsByKind: [\n            null as Record<string, CampaignMetricsDetails> | null,\n            {\n                loadMetricsByKind: async () => {\n                    const { before, after, interval } = values.filters\n\n                    const dateClause =\n                        interval === 'day'\n                            ? 'toStartOfDay(timestamp)'\n                            : interval === 'week'\n                            ? 'toStartOfWeek(timestamp)'\n                            : 'toStartOfHour(timestamp)'\n\n                    const query = hogql`SELECT ${hogql.raw(\n                        dateClause\n                    )} AS timestamp, metric_name, count() AS total_count\n                        FROM app_metrics\n                        WHERE app_source = 'hog_flow'\n                        AND app_source_id = ${props.id}\n                        AND timestamp >= {filters.dateRange.from}\n                        AND timestamp <= {filters.dateRange.to}\n                        GROUP BY timestamp, metric_name\n                        ORDER BY timestamp, metric_name`\n\n                    const response = await api.queryHogQL(query, {\n                        refresh: 'force_blocking',\n                        filtersOverride: {\n                            date_from: after ?? '-7d',\n                            date_to: before,\n                        },\n                    })\n\n                    const byKind: Record<string, CampaignMetricsDetails> = {}\n\n                    // TODO: The results don't include empty values in the sense that they aren't creating a full time series.\n                    for (const result of response.results) {\n                        const [time, name, count] = result\n\n                        if (!byKind[name]) {\n                            byKind[name] = {\n                                name,\n                                labels: [],\n                                values: [],\n                                total: 0,\n                            }\n                        }\n\n                        byKind[name].labels.push(time)\n                        byKind[name].values.push(count)\n                        byKind[name].total += count\n                    }\n\n                    return byKind\n                },\n            },\n        ],\n    })),\n    reducers({\n        filters: [\n            DEFAULT_FILTERS,\n            {\n                setFilters: (state, { filters }) => ({ ...state, ...filters }),\n            },\n        ],\n    }),\n    listeners(({ actions }) => ({\n        setFilters: async (_, breakpoint) => {\n            await breakpoint(100)\n            actions.loadMetricsByKind()\n        },\n    })),\n])\n"],"names":[],"sourceRoot":""}