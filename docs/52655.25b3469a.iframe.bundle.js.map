{"version":3,"file":"52655.25b3469a.iframe.bundle.js","mappings":";AAiDA;AACA;AACA;AACA;AACA;AACA;ACDA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://@posthog/storybook/../../frontend/src/scenes/pipeline/hogfunctions/logs/hogFunctionLogsLogic.ts","webpack://@posthog/storybook/../../frontend/src/scenes/pipeline/hogfunctions/logs/logsViewerLogic.tsx"],"sourcesContent":["import { lemonToast } from '@posthog/lemon-ui'\nimport { actions, connect, kea, key, listeners, path, props, reducers, selectors } from 'kea'\nimport { beforeUnload } from 'kea-router'\nimport api from 'lib/api'\nimport { Dayjs, dayjs } from 'lib/dayjs'\n\nimport { HogQLQuery, NodeKind } from '~/queries/schema/schema-general'\nimport { LogEntryLevel } from '~/types'\n\nimport type { hogFunctionLogsLogicType } from './hogFunctionLogsLogicType'\nimport { GroupedLogEntry, logsViewerLogic, LogsViewerLogicProps } from './logsViewerLogic'\n\nexport type RetryInvocationState = 'pending' | 'success' | 'failure'\n\nconst eventIdMatchers = [/Event: ([A-Za-z0-9-]+)/, /\\/events\\/([A-Za-z0-9-]+)\\//, /event ([A-Za-z0-9-]+)/]\n\nasync function runWithParallelism<T, R>(\n    items: T[],\n    maxParallel: number,\n    asyncFn: (item: T) => Promise<R>\n): Promise<R[]> {\n    const results: R[] = []\n    const executing = new Set<Promise<void>>()\n\n    for (const item of items) {\n        const promise = (async () => {\n            const result = await asyncFn(item)\n            results.push(result)\n        })()\n\n        executing.add(promise)\n        void promise.finally(() => executing.delete(promise))\n\n        if (executing.size >= maxParallel) {\n            await Promise.race(executing)\n        }\n    }\n\n    await Promise.all(executing)\n    return results\n}\n\nconst loadClickhouseEvents = async (\n    eventIds: string[],\n    { date_from, date_to }: { date_from?: string; date_to?: string }\n): Promise<any[]> => {\n    const query: HogQLQuery = {\n        kind: NodeKind.HogQLQuery,\n        query: `\n            SELECT uuid, distinct_id, event, timestamp, properties, elements_chain, person.id, person.properties, person.created_at \n            FROM events\n            WHERE uuid in (${eventIds.map((x) => `'${x}'`).join(',')})\n            AND timestamp > {filters.dateRange.from}\n            AND timestamp < {filters.dateRange.to}\n        `,\n    }\n\n    const response = await api.query(query, undefined, undefined, 'force_blocking', {\n        date_from: date_from,\n        date_to: date_to,\n    })\n\n    return response.results.map((x) => {\n        const [\n            uuid,\n            distinct_id,\n            event,\n            timestamp,\n            properties,\n            elements_chain,\n            person_id,\n            person_properties,\n            person_created_at,\n        ] = x\n\n        return {\n            uuid,\n            event,\n            distinct_id,\n            person_id,\n            timestamp,\n            properties,\n            elements_chain,\n            person_created_at,\n            person_properties,\n        }\n    })\n}\n\nexport const hogFunctionLogsLogic = kea<hogFunctionLogsLogicType>([\n    path((key) => ['scenes', 'pipeline', 'hogfunctions', 'logs', 'hogFunctionLogsLogic', key]),\n    props({} as LogsViewerLogicProps), // TODO: Remove `stage` from props, it isn't needed here for anything\n    key(({ sourceType, sourceId }) => `${sourceType}:${sourceId}`),\n    connect((props: LogsViewerLogicProps) => ({\n        values: [logsViewerLogic(props), ['logs']],\n        actions: [logsViewerLogic(props), ['addLogGroups', 'setRowExpanded']],\n    })),\n    actions({\n        setSelectingMany: (selectingMany: boolean) => ({ selectingMany }),\n        setSelectedForRetry: (selectedForRetry: Record<string, boolean>) => ({ selectedForRetry }),\n        selectAllForRetry: true,\n        retryInvocation: (groupedLogEntry: GroupedLogEntry, eventId: string) => ({ groupedLogEntry, eventId }),\n        retryInvocations: (groupedLogEntries: GroupedLogEntry[]) => ({ groupedLogEntries }),\n        retryInvocationStarted: (groupedLogEntry: GroupedLogEntry) => ({ groupedLogEntry }),\n        retryInvocationSuccess: (groupedLogEntry: GroupedLogEntry) => ({ groupedLogEntry }),\n        retryInvocationFailure: (groupedLogEntry: GroupedLogEntry) => ({ groupedLogEntry }),\n        retrySelectedInvocations: true,\n    }),\n    reducers({\n        selectingMany: [\n            false,\n            {\n                setSelectingMany: (_, { selectingMany }) => selectingMany,\n            },\n        ],\n\n        selectedForRetry: [\n            {} as Record<string, boolean>,\n            {\n                setSelectedForRetry: (state, { selectedForRetry }) => {\n                    const newState = { ...state }\n                    Object.keys(selectedForRetry).forEach((key) => {\n                        newState[key] = selectedForRetry[key]\n\n                        if (!selectedForRetry[key]) {\n                            delete newState[key]\n                        }\n                    })\n                    return newState\n                },\n\n                setSelectingMany: (state, { selectingMany }) => {\n                    return selectingMany ? state : {}\n                },\n            },\n        ],\n\n        retries: [\n            {} as Record<string, RetryInvocationState>,\n            {\n                retryInvocationStarted: (state, { groupedLogEntry }) => {\n                    return {\n                        ...state,\n                        [groupedLogEntry.instanceId]: 'pending',\n                    }\n                },\n\n                retryInvocationSuccess: (state, { groupedLogEntry }) => {\n                    return {\n                        ...state,\n                        [groupedLogEntry.instanceId]: 'success',\n                    }\n                },\n\n                retryInvocationFailure: (state, { groupedLogEntry }) => {\n                    return {\n                        ...state,\n                        [groupedLogEntry.instanceId]: 'failure',\n                    }\n                },\n            },\n        ],\n    }),\n\n    selectors({\n        retryRunning: [\n            (s) => [s.retries],\n            (retries) => {\n                return Object.values(retries).some((x) => x === 'pending')\n            },\n        ],\n\n        eventIdByInvocationId: [\n            (s) => [s.logs],\n            (logs) => {\n                const eventIdByInvocationId: Record<string, string> = {}\n\n                for (const record of logs) {\n                    // TRICKY: We have the event ID in different places in different logs. We will standardise this to be the invocation ID in the future.\n                    const entryContainingEventId = record.entries.find(\n                        (entry) =>\n                            entry.message.includes('Function completed') ||\n                            entry.message.includes('Suspending function') ||\n                            entry.message.includes('Error executing function on event')\n                    )\n\n                    if (!entryContainingEventId) {\n                        return undefined\n                    }\n\n                    for (const matcher of eventIdMatchers) {\n                        const match = entryContainingEventId.message.match(matcher)\n                        if (match) {\n                            eventIdByInvocationId[record.instanceId] = match[1]\n                            break\n                        }\n                    }\n                }\n\n                return eventIdByInvocationId\n            },\n        ],\n    }),\n    listeners(({ actions, props, values }) => ({\n        retryInvocations: async ({ groupedLogEntries }) => {\n            await lemonToast.promise(\n                (async () => {\n                    for (const groupedLogEntry of groupedLogEntries) {\n                        actions.retryInvocationStarted(groupedLogEntry)\n                    }\n\n                    if (groupedLogEntries.length === 1) {\n                        // If we only have one log group then we can just expand it to be a little more user friendly\n                        actions.setRowExpanded(groupedLogEntries[0].instanceId, true)\n                    }\n\n                    // We want to get the oldest and newest \"min\" timestamp as that will be closest to when the event was processed\n                    // NOTE: This isn't perfect as the event timestamp might be different to the time it was processed\n                    const [timestampRangeStart, timestampRangeEnd] = groupedLogEntries.reduce(\n                        ([accStart, accEnd], x) => {\n                            if (!accStart) {\n                                return [x.minTimestamp, x.minTimestamp]\n                            }\n\n                            return [\n                                x.minTimestamp.isBefore(accStart) ? x.minTimestamp : accStart,\n                                x.maxTimestamp.isAfter(accEnd) ? x.maxTimestamp : accEnd,\n                            ]\n                        },\n                        [null as Dayjs | null, null as Dayjs | null]\n                    )\n\n                    // Load all events by ID using the date range to speed up the query (we add time either side to account for processing delays)\n                    const events = await loadClickhouseEvents(Object.values(values.eventIdByInvocationId ?? {}), {\n                        date_from: timestampRangeStart?.subtract(1, 'day').toISOString(),\n                        date_to: timestampRangeEnd?.add(1, 'day').toISOString(),\n                    })\n\n                    const eventsById: Record<string, any> = {}\n                    for (const event of events) {\n                        eventsById[event.uuid] = event\n                    }\n\n                    await runWithParallelism(groupedLogEntries, 10, async (groupedLogEntry) => {\n                        try {\n                            // If we have an event then retry it, otherwise fail\n                            const event = eventsById[values.eventIdByInvocationId![groupedLogEntry.instanceId]]\n\n                            if (!event) {\n                                actions.retryInvocationFailure(groupedLogEntry)\n                                return\n                            }\n\n                            const res = await api.hogFunctions.createTestInvocation(props.sourceId, {\n                                clickhouse_event: event,\n                                mock_async_functions: false,\n                                configuration: {\n                                    // For retries we don't care about filters\n                                    filters: {},\n                                },\n                                invocation_id: groupedLogEntry.instanceId,\n                            })\n\n                            const newLogGroup: GroupedLogEntry = {\n                                ...groupedLogEntry,\n                                entries: [\n                                    ...groupedLogEntry.entries,\n                                    ...res.logs.map((x) => ({\n                                        timestamp: dayjs(x.timestamp),\n                                        level: x.level.toUpperCase() as LogEntryLevel,\n                                        message: x.message,\n                                    })),\n                                ],\n                            }\n\n                            actions.addLogGroups([newLogGroup])\n                            actions.retryInvocationSuccess(groupedLogEntry)\n                        } catch (e) {\n                            actions.retryInvocationFailure(groupedLogEntry)\n                        }\n                    })\n\n                    actions.setSelectingMany(false)\n                })(),\n                {\n                    success: 'Retries complete!',\n                    error: 'Retry failed!',\n                    pending: 'Retrying...',\n                }\n            )\n        },\n\n        retrySelectedInvocations: async () => {\n            const groupsToRetry = values.logs.filter((x) => values.selectedForRetry[x.instanceId])\n\n            actions.retryInvocations(groupsToRetry)\n        },\n\n        selectAllForRetry: async () => {\n            actions.setSelectingMany(true)\n\n            for (const groupedLogEntry of values.logs) {\n                actions.setSelectedForRetry({\n                    [groupedLogEntry.instanceId]: true,\n                })\n            }\n        },\n    })),\n\n    beforeUnload(({ values, cache }) => ({\n        enabled: () => !cache.disabledBeforeUnload && values.retryRunning,\n        message: 'You have running retries that will be discarded if you leave. Are you sure?',\n        onConfirm: () => {\n            cache.disabledBeforeUnload = true\n        },\n    })),\n])\n","import { actions, events, kea, key, listeners, path, props, reducers, selectors } from 'kea'\nimport { loaders } from 'kea-loaders'\nimport api from 'lib/api'\nimport { Dayjs, dayjs } from 'lib/dayjs'\n\nimport { HogQLQuery, NodeKind } from '~/queries/schema/schema-general'\nimport { LogEntryLevel } from '~/types'\n\nimport type { logsViewerLogicType } from './logsViewerLogicType'\n\nexport const ALL_LOG_LEVELS: LogEntryLevel[] = ['DEBUG', 'LOG', 'INFO', 'WARNING', 'ERROR']\nexport const DEFAULT_LOG_LEVELS: LogEntryLevel[] = ['LOG', 'INFO', 'WARNING', 'ERROR']\n\nexport type LogsViewerLogicProps = {\n    sourceType: 'hog_function'\n    sourceId: string\n}\n\nexport type LogsViewerFilters = {\n    levels: LogEntryLevel[]\n    search: string\n    date_from?: string\n    date_to?: string\n}\n\nexport const LOG_VIEWER_LIMIT = 500\n\nexport type GroupedLogEntry = {\n    instanceId: string\n    maxTimestamp: Dayjs\n    minTimestamp: Dayjs\n    logLevel: LogEntryLevel\n    entries: {\n        message: string\n        level: LogEntryLevel\n        timestamp: Dayjs\n    }[]\n}\n\ntype GroupedLogEntryRequest = {\n    sourceType: 'hog_function'\n    sourceId: string\n    levels: LogEntryLevel[]\n    search: string\n    date_from?: string\n    date_to?: string\n    order: 'ASC' | 'DESC'\n}\n\nconst loadGroupedLogs = async (request: GroupedLogEntryRequest): Promise<GroupedLogEntry[]> => {\n    const query: HogQLQuery = {\n        kind: NodeKind.HogQLQuery,\n        query: `SELECT\n            instance_id,\n            max(timestamp) AS latest_timestamp,\n            min(timestamp) AS earliest_timestamp,\n            arraySort(\n                groupArray((timestamp, level, message))\n            ) AS messages\n        FROM log_entries\n        WHERE log_source = '${request.sourceType}'\n        AND log_source_id = '${request.sourceId}'\n        AND timestamp > {filters.dateRange.from}\n        AND timestamp < {filters.dateRange.to}\n        AND instance_id in (\n            SELECT DISTINCT instance_id\n            FROM log_entries\n            WHERE log_source = 'hog_function'\n            AND log_source_id = '${request.sourceId}'\n            AND timestamp > {filters.dateRange.from}\n            AND timestamp < {filters.dateRange.to}\n            AND lower(level) IN (${request.levels.map((level) => `'${level.toLowerCase()}'`).join(',')})\n            AND message ILIKE '%${request.search}%'\n            ORDER BY timestamp ${request.order}\n            LIMIT ${LOG_VIEWER_LIMIT}\n        )\n        GROUP BY instance_id\n        ORDER BY latest_timestamp DESC`,\n    }\n\n    const response = await api.query(query, undefined, undefined, 'force_blocking', {\n        date_from: request.date_from ?? '-7d',\n        date_to: request.date_to,\n    })\n\n    return response.results.map((result) => ({\n        instanceId: result[0],\n        maxTimestamp: dayjs(result[1]),\n        minTimestamp: dayjs(result[2]),\n        entries: result[3].map((entry: any) => ({\n            timestamp: dayjs(entry[0]),\n            level: entry[1].toUpperCase(),\n            message: entry[2],\n        })),\n    })) as GroupedLogEntry[]\n}\n\nconst sanitizeGroupedLogs = (groups: GroupedLogEntry[]): GroupedLogEntry[] => {\n    const byId: Record<string, GroupedLogEntry> = {}\n\n    for (const group of groups) {\n        // Set the group if not already set\n        if (!byId[group.instanceId]) {\n            byId[group.instanceId] = group\n        } else {\n            // If the group already exists, we need to merge the entries\n            for (const entry of group.entries) {\n                if (!byId[group.instanceId].entries.find((e) => e.timestamp.isSame(entry.timestamp))) {\n                    byId[group.instanceId].entries.push(entry)\n                }\n            }\n        }\n\n        // Sort the entries by timestamp\n        byId[group.instanceId].entries.sort((a, b) => a.timestamp.diff(b.timestamp))\n\n        // Go in reverse and find the highest level message\n\n        const highestLogLevel = group.entries.reduce((max, entry) => {\n            return Math.max(max, ALL_LOG_LEVELS.indexOf(entry.level))\n        }, 0)\n        byId[group.instanceId].logLevel = ALL_LOG_LEVELS[highestLogLevel]\n    }\n\n    return Object.values(byId).sort((a, b) => b.maxTimestamp.diff(a.maxTimestamp))\n}\n\nexport const logsViewerLogic = kea<logsViewerLogicType>([\n    path((key) => ['scenes', 'pipeline', 'hogfunctions', 'logs', 'logsViewerLogic', key]),\n    props({} as LogsViewerLogicProps), // TODO: Remove `stage` from props, it isn't needed here for anything\n    key(({ sourceType, sourceId }) => `${sourceType}:${sourceId}`),\n    actions({\n        setFilters: (filters: Partial<LogsViewerFilters>) => ({ filters }),\n        addLogGroups: (logGroups: GroupedLogEntry[]) => ({ logGroups }),\n        setHiddenLogs: (logGroups: GroupedLogEntry[]) => ({ logGroups }),\n        clearHiddenLogs: true,\n        markLogsEnd: true,\n        revealHiddenLogs: true,\n        setRowExpanded: (instanceId: string, expanded: boolean) => ({ instanceId, expanded }),\n        scheduleLoadNewerLogs: true,\n        loadLogs: true,\n        loadNewerLogs: true,\n    }),\n    loaders(({ props, values, actions }) => ({\n        logs: [\n            [] as GroupedLogEntry[],\n            {\n                loadLogs: async (_, breakpoint) => {\n                    await breakpoint(10)\n\n                    actions.clearHiddenLogs()\n\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_from: values.filters.date_from,\n                        date_to: values.filters.date_to,\n                        order: 'DESC',\n                    }\n                    const results = await loadGroupedLogs(logParams)\n\n                    await breakpoint(10)\n\n                    return sanitizeGroupedLogs(results)\n                },\n                loadMoreLogs: async () => {\n                    if (!values.oldestLogTimestamp) {\n                        return values.logs\n                    }\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_to: values.oldestLogTimestamp.toISOString(),\n                        date_from: values.filters.date_from,\n                        order: 'DESC',\n                    }\n\n                    const results = await loadGroupedLogs(logParams)\n\n                    if (!results.length) {\n                        actions.markLogsEnd()\n                    }\n                    return sanitizeGroupedLogs([...results, ...values.logs])\n                },\n\n                revealHiddenLogs: () => {\n                    // We pull out the hidden log groups and add them to the main logs\n                    const hiddenLogs = [...values.hiddenLogs]\n\n                    actions.clearHiddenLogs()\n                    return sanitizeGroupedLogs([...hiddenLogs, ...values.logs])\n                },\n                addLogGroups: ({ logGroups }) => {\n                    return sanitizeGroupedLogs([...logGroups, ...values.logs])\n                },\n            },\n        ],\n\n        hiddenLogs: [\n            [] as GroupedLogEntry[],\n            {\n                loadNewerLogs: async (_, breakpoint) => {\n                    await breakpoint(10)\n\n                    // We load all logs groups that have a timestamp after the newest log timestamp\n                    // For ones we already have we just replace them, otherwise we add them to the \"hidden\" logs list\n                    if (!values.newestLogTimestamp) {\n                        return values.hiddenLogs\n                    }\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_from: values.newestLogTimestamp.toISOString(),\n                        date_to: values.filters.date_to,\n                        order: 'ASC',\n                    }\n\n                    const results = await loadGroupedLogs(logParams)\n\n                    await breakpoint(10)\n\n                    const newLogs: GroupedLogEntry[] = []\n                    const existingLogsToUpdate: GroupedLogEntry[] = []\n                    const existingLogIds = values.logs.map((log) => log.instanceId)\n\n                    if (values.logsLoading) {\n                        // TRICKY: Something changed whilst we were doing this query - we don't want to mess with things\n                        // so we just exit\n                        return values.hiddenLogs\n                    }\n\n                    for (const log of results) {\n                        if (existingLogIds.includes(log.instanceId)) {\n                            // If we already have this log group showing then we can just update it\n                            existingLogsToUpdate.push(log)\n                        } else {\n                            // Otherwise we add it to the list of hidden logs\n                            newLogs.push(log)\n                        }\n                    }\n\n                    if (existingLogsToUpdate.length) {\n                        // Update the existing logs with the new data\n                        actions.loadLogsSuccess(sanitizeGroupedLogs([...existingLogsToUpdate, ...values.logs]))\n                    }\n\n                    actions.scheduleLoadNewerLogs()\n\n                    return sanitizeGroupedLogs([...newLogs, ...values.hiddenLogs])\n                },\n                clearHiddenLogs: () => [],\n            },\n        ],\n    })),\n    reducers({\n        filters: [\n            {\n                search: '',\n                levels: DEFAULT_LOG_LEVELS,\n                date_from: '-7d',\n                date_to: undefined,\n            } as LogsViewerFilters,\n            {\n                setFilters: (state, { filters }) => ({\n                    ...state,\n                    ...filters,\n                }),\n            },\n        ],\n        isThereMoreToLoad: [\n            true,\n            {\n                markLogsEnd: () => false,\n                loadLogs: () => true,\n            },\n        ],\n        expandedRows: [\n            {} as Record<string, boolean>,\n            {\n                setRowExpanded: (state, { instanceId, expanded }) => ({\n                    ...state,\n                    [instanceId]: expanded,\n                }),\n            },\n        ],\n    }),\n    selectors(() => ({\n        newestLogTimestamp: [\n            (s) => [s.logs, s.hiddenLogs],\n            (logs: GroupedLogEntry[], hiddenLogs: GroupedLogEntry[]): Dayjs | null => {\n                return logs.concat(hiddenLogs).reduce((max, log) => {\n                    if (!max) {\n                        return log.maxTimestamp\n                    }\n                    return log.maxTimestamp.isAfter(max) ? log.maxTimestamp : max\n                }, null as Dayjs | null)\n            },\n        ],\n\n        oldestLogTimestamp: [\n            (s) => [s.logs, s.hiddenLogs],\n            (logs: GroupedLogEntry[], hiddenLogs: GroupedLogEntry[]): Dayjs | null => {\n                return logs.concat(hiddenLogs).reduce((min, log) => {\n                    if (!min) {\n                        return log.minTimestamp\n                    }\n                    return log.minTimestamp.isBefore(min) ? log.minTimestamp : min\n                }, null as Dayjs | null)\n            },\n        ],\n    })),\n    listeners(({ actions, cache }) => ({\n        setFilters: async (_, breakpoint) => {\n            await breakpoint(500)\n            actions.loadLogs()\n        },\n\n        loadLogsSuccess: () => {\n            actions.scheduleLoadNewerLogs()\n        },\n\n        scheduleLoadNewerLogs: () => {\n            if (cache.pollingTimeout) {\n                clearTimeout(cache.pollingTimeout)\n            }\n            cache.pollingTimeout = setTimeout(() => actions.loadNewerLogs(), 5000)\n        },\n    })),\n    events(({ actions, cache }) => ({\n        afterMount: () => {\n            actions.loadLogs()\n        },\n        beforeUnmount: () => {\n            clearInterval(cache.pollingTimeout)\n        },\n    })),\n])\n"],"names":[],"sourceRoot":""}