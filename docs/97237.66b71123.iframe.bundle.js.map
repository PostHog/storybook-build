{"version":3,"file":"97237.66b71123.iframe.bundle.js","mappings":";;;;;;;;;AA2DA;AACA;;;;;;AAMA;AACA;;;AAGA;AACA;AACA;AACA;;;AAGA","sources":["webpack://@posthog/storybook/../../frontend/src/scenes/hog-functions/logs/logsViewerLogic.tsx"],"sourcesContent":["import { actions, events, kea, key, listeners, path, props, reducers, selectors } from 'kea'\nimport { loaders } from 'kea-loaders'\nimport api from 'lib/api'\nimport { Dayjs, dayjs } from 'lib/dayjs'\n\nimport { hogql } from '~/queries/utils'\nimport { LogEntryLevel } from '~/types'\n\nimport type { logsViewerLogicType } from './logsViewerLogicType'\n\nexport const ALL_LOG_LEVELS: LogEntryLevel[] = ['DEBUG', 'LOG', 'INFO', 'WARNING', 'ERROR']\nexport const DEFAULT_LOG_LEVELS: LogEntryLevel[] = ['LOG', 'INFO', 'WARNING', 'ERROR']\n\nexport type LogsViewerLogicProps = {\n    sourceType: 'hog_function' | 'hog_flow'\n    sourceId: string\n}\n\nexport type LogsViewerFilters = {\n    levels: LogEntryLevel[]\n    search: string\n    date_from?: string\n    date_to?: string\n}\n\nexport const LOG_VIEWER_LIMIT = 500\n\nexport type GroupedLogEntry = {\n    instanceId: string\n    maxTimestamp: Dayjs\n    minTimestamp: Dayjs\n    logLevel: LogEntryLevel\n    entries: {\n        message: string\n        level: LogEntryLevel\n        timestamp: Dayjs\n    }[]\n}\n\ntype GroupedLogEntryRequest = {\n    sourceType: 'hog_function' | 'hog_flow'\n    sourceId: string\n    levels: LogEntryLevel[]\n    search: string\n    date_from?: string\n    date_to?: string\n    order: 'ASC' | 'DESC'\n}\n\nconst loadGroupedLogs = async (request: GroupedLogEntryRequest): Promise<GroupedLogEntry[]> => {\n    const query = hogql`\n        SELECT\n            instance_id,\n            max(timestamp) AS latest_timestamp,\n            min(timestamp) AS earliest_timestamp,\n            arraySort(\n                groupArray((timestamp, level, message))\n            ) AS messages\n        FROM log_entries\n        WHERE log_source = ${request.sourceType}\n        AND log_source_id = ${request.sourceId}\n        AND timestamp > {filters.dateRange.from}\n        AND timestamp < {filters.dateRange.to}\n        AND instance_id in (\n            SELECT DISTINCT instance_id\n            FROM log_entries\n            WHERE log_source = ${request.sourceType}\n            AND log_source_id = ${request.sourceId}\n            AND timestamp > {filters.dateRange.from}\n            AND timestamp < {filters.dateRange.to}\n            AND lower(level) IN (${hogql.raw(request.levels.map((level) => `'${level.toLowerCase()}'`).join(','))})\n            AND message ILIKE '%${hogql.raw(request.search)}%'\n            ORDER BY timestamp ${hogql.raw(request.order)}\n            LIMIT ${LOG_VIEWER_LIMIT}\n        )\n        GROUP BY instance_id\n        ORDER BY latest_timestamp DESC`\n\n    const response = await api.queryHogQL(query, {\n        refresh: 'force_blocking',\n        filtersOverride: {\n            date_from: request.date_from ?? '-7d',\n            date_to: request.date_to,\n        },\n    })\n\n    return response.results.map((result) => ({\n        instanceId: result[0],\n        maxTimestamp: dayjs(result[1]),\n        minTimestamp: dayjs(result[2]),\n        entries: result[3].map((entry: any) => ({\n            timestamp: dayjs(entry[0]),\n            level: entry[1].toUpperCase(),\n            message: entry[2],\n        })),\n    })) as GroupedLogEntry[]\n}\n\nconst sanitizeGroupedLogs = (groups: GroupedLogEntry[]): GroupedLogEntry[] => {\n    const byId: Record<string, GroupedLogEntry> = {}\n\n    for (const group of groups) {\n        // Set the group if not already set\n        if (!byId[group.instanceId]) {\n            byId[group.instanceId] = group\n        } else {\n            // If the group already exists, we need to merge the entries\n            for (const entry of group.entries) {\n                if (!byId[group.instanceId].entries.find((e) => e.timestamp.isSame(entry.timestamp))) {\n                    byId[group.instanceId].entries.push(entry)\n                }\n            }\n        }\n\n        // Sort the entries by timestamp\n        byId[group.instanceId].entries.sort((a, b) => a.timestamp.diff(b.timestamp))\n\n        // Go in reverse and find the highest level message\n\n        const highestLogLevel = group.entries.reduce((max, entry) => {\n            return Math.max(max, ALL_LOG_LEVELS.indexOf(entry.level))\n        }, 0)\n        byId[group.instanceId].logLevel = ALL_LOG_LEVELS[highestLogLevel]\n    }\n\n    return Object.values(byId).sort((a, b) => b.maxTimestamp.diff(a.maxTimestamp))\n}\n\nexport const logsViewerLogic = kea<logsViewerLogicType>([\n    path((key) => ['scenes', 'pipeline', 'hogfunctions', 'logs', 'logsViewerLogic', key]),\n    props({} as LogsViewerLogicProps), // TODO: Remove `stage` from props, it isn't needed here for anything\n    key(({ sourceType, sourceId }) => `${sourceType}:${sourceId}`),\n    actions({\n        setFilters: (filters: Partial<LogsViewerFilters>) => ({ filters }),\n        addLogGroups: (logGroups: GroupedLogEntry[]) => ({ logGroups }),\n        setHiddenLogs: (logGroups: GroupedLogEntry[]) => ({ logGroups }),\n        clearHiddenLogs: true,\n        markLogsEnd: true,\n        revealHiddenLogs: true,\n        setRowExpanded: (instanceId: string, expanded: boolean) => ({ instanceId, expanded }),\n        scheduleLoadNewerLogs: true,\n        loadLogs: true,\n        loadNewerLogs: true,\n    }),\n    loaders(({ props, values, actions }) => ({\n        logs: [\n            [] as GroupedLogEntry[],\n            {\n                loadLogs: async (_, breakpoint) => {\n                    await breakpoint(10)\n\n                    actions.clearHiddenLogs()\n\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_from: values.filters.date_from,\n                        date_to: values.filters.date_to,\n                        order: 'DESC',\n                    }\n                    const results = await loadGroupedLogs(logParams)\n\n                    await breakpoint(10)\n\n                    return sanitizeGroupedLogs(results)\n                },\n                loadMoreLogs: async () => {\n                    if (!values.oldestLogTimestamp) {\n                        return values.logs\n                    }\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_to: values.oldestLogTimestamp.toISOString(),\n                        date_from: values.filters.date_from,\n                        order: 'DESC',\n                    }\n\n                    const results = await loadGroupedLogs(logParams)\n\n                    if (!results.length) {\n                        actions.markLogsEnd()\n                    }\n                    return sanitizeGroupedLogs([...results, ...values.logs])\n                },\n\n                revealHiddenLogs: () => {\n                    // We pull out the hidden log groups and add them to the main logs\n                    const hiddenLogs = [...values.hiddenLogs]\n\n                    actions.clearHiddenLogs()\n                    return sanitizeGroupedLogs([...hiddenLogs, ...values.logs])\n                },\n                addLogGroups: ({ logGroups }) => {\n                    return sanitizeGroupedLogs([...logGroups, ...values.logs])\n                },\n            },\n        ],\n\n        hiddenLogs: [\n            [] as GroupedLogEntry[],\n            {\n                loadNewerLogs: async (_, breakpoint) => {\n                    await breakpoint(10)\n\n                    // We load all logs groups that have a timestamp after the newest log timestamp\n                    // For ones we already have we just replace them, otherwise we add them to the \"hidden\" logs list\n                    if (!values.newestLogTimestamp) {\n                        return values.hiddenLogs\n                    }\n                    const logParams: GroupedLogEntryRequest = {\n                        levels: values.filters.levels,\n                        search: values.filters.search,\n                        sourceType: props.sourceType,\n                        sourceId: props.sourceId,\n                        date_from: values.newestLogTimestamp.toISOString(),\n                        date_to: values.filters.date_to,\n                        order: 'ASC',\n                    }\n\n                    const results = await loadGroupedLogs(logParams)\n\n                    await breakpoint(10)\n\n                    const newLogs: GroupedLogEntry[] = []\n                    const existingLogsToUpdate: GroupedLogEntry[] = []\n                    const existingLogIds = values.logs.map((log) => log.instanceId)\n\n                    if (values.logsLoading) {\n                        // TRICKY: Something changed whilst we were doing this query - we don't want to mess with things\n                        // so we just exit\n                        return values.hiddenLogs\n                    }\n\n                    for (const log of results) {\n                        if (existingLogIds.includes(log.instanceId)) {\n                            // If we already have this log group showing then we can just update it\n                            existingLogsToUpdate.push(log)\n                        } else {\n                            // Otherwise we add it to the list of hidden logs\n                            newLogs.push(log)\n                        }\n                    }\n\n                    if (existingLogsToUpdate.length) {\n                        // Update the existing logs with the new data\n                        actions.loadLogsSuccess(sanitizeGroupedLogs([...existingLogsToUpdate, ...values.logs]))\n                    }\n\n                    actions.scheduleLoadNewerLogs()\n\n                    return sanitizeGroupedLogs([...newLogs, ...values.hiddenLogs])\n                },\n                clearHiddenLogs: () => [],\n            },\n        ],\n    })),\n    reducers({\n        filters: [\n            {\n                search: '',\n                levels: DEFAULT_LOG_LEVELS,\n                date_from: '-7d',\n                date_to: undefined,\n            } as LogsViewerFilters,\n            {\n                setFilters: (state, { filters }) => ({\n                    ...state,\n                    ...filters,\n                }),\n            },\n        ],\n        isThereMoreToLoad: [\n            true,\n            {\n                markLogsEnd: () => false,\n                loadLogs: () => true,\n            },\n        ],\n        expandedRows: [\n            {} as Record<string, boolean>,\n            {\n                setRowExpanded: (state, { instanceId, expanded }) => ({\n                    ...state,\n                    [instanceId]: expanded,\n                }),\n            },\n        ],\n    }),\n    selectors(() => ({\n        newestLogTimestamp: [\n            (s) => [s.logs, s.hiddenLogs],\n            (logs: GroupedLogEntry[], hiddenLogs: GroupedLogEntry[]): Dayjs | null => {\n                return logs.concat(hiddenLogs).reduce((max, log) => {\n                    if (!max) {\n                        return log.maxTimestamp\n                    }\n                    return log.maxTimestamp.isAfter(max) ? log.maxTimestamp : max\n                }, null as Dayjs | null)\n            },\n        ],\n\n        oldestLogTimestamp: [\n            (s) => [s.logs, s.hiddenLogs],\n            (logs: GroupedLogEntry[], hiddenLogs: GroupedLogEntry[]): Dayjs | null => {\n                return logs.concat(hiddenLogs).reduce((min, log) => {\n                    if (!min) {\n                        return log.minTimestamp\n                    }\n                    return log.minTimestamp.isBefore(min) ? log.minTimestamp : min\n                }, null as Dayjs | null)\n            },\n        ],\n    })),\n    listeners(({ actions, cache }) => ({\n        setFilters: async (_, breakpoint) => {\n            await breakpoint(500)\n            actions.loadLogs()\n        },\n\n        loadLogsSuccess: () => {\n            actions.scheduleLoadNewerLogs()\n        },\n\n        scheduleLoadNewerLogs: () => {\n            if (cache.pollingTimeout) {\n                clearTimeout(cache.pollingTimeout)\n            }\n            cache.pollingTimeout = setTimeout(() => actions.loadNewerLogs(), 5000)\n        },\n    })),\n    events(({ actions, cache }) => ({\n        afterMount: () => {\n            actions.loadLogs()\n        },\n        beforeUnmount: () => {\n            clearInterval(cache.pollingTimeout)\n        },\n    })),\n])\n"],"names":[],"sourceRoot":""}